# services/gemini_client.py
"""
Gemini API í´ë¼ì´ì–¸íŠ¸ - LangChain í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤ + ì§ì ‘ REST API

ì´ íŒŒì¼ì€ ë‘ ê°€ì§€ ì ‘ê·¼ ë°©ì‹ì„ ê²°í•©í•©ë‹ˆë‹¤:
1. LangChain í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤ (Message, Memory ë“±)
2. ì§ì ‘ REST API í˜¸ì¶œ (SSL ìš°íšŒ - requests with verify=False)

LangChain SDKë¥¼ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ì´ìœ :
- ìì²´ ì„œëª… ì¸ì¦ì„œ í™˜ê²½ì—ì„œ SSL ê²€ì¦ ìš°íšŒê°€ ë¶ˆê°€ëŠ¥
- google-generativeai SDK ë‚´ë¶€ HTTP í´ë¼ì´ì–¸íŠ¸ëŠ” verify=False ì§€ì› ì•ˆ í•¨

í•´ê²° ë°©ë²•:
- LangChainì˜ ì¶”ìƒí™” (Message, Memory ë“±)ë¥¼ ì‚¬ìš©í•˜ë˜
- ì‹¤ì œ API í˜¸ì¶œì€ requests ë¼ì´ë¸ŒëŸ¬ë¦¬ ì§ì ‘ ì‚¬ìš© (verify=False)

í•„ìš”í•œ íŒ¨í‚¤ì§€:
pip install langchain langchain-core requests

í™˜ê²½ ë³€ìˆ˜:
- GOOGLE_API_KEY: Google AI Studio API í‚¤
- GEMINI_MODEL: ëª¨ë¸ëª… (ê¸°ë³¸ê°’: gemini-2.5-flash)
"""

import os
import logging
import asyncio
import warnings
import re
import time
import requests
from typing import Dict, Any, Optional, List, Callable, Awaitable, Tuple

# SSL ê²½ê³  ë¹„í™œì„±í™”
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
warnings.filterwarnings('ignore', category=DeprecationWarning)

logger = logging.getLogger(__name__)

# =============================================================================
# LangChain ì„í¬íŠ¸ (ì¸í„°í˜ì´ìŠ¤ ë° ë©”ëª¨ë¦¬ ê´€ë¦¬ìš©)
# =============================================================================
# [LangChain ì‚¬ìš©] ë©”ì‹œì§€ íƒ€ì… - í‘œì¤€í™”ëœ ë©”ì‹œì§€ ì¸í„°í˜ì´ìŠ¤
from langchain_core.messages import (
    HumanMessage, 
    AIMessage, 
    SystemMessage, 
    BaseMessage
)
# [LangChain ì‚¬ìš©] ëŒ€í™” ë©”ëª¨ë¦¬ - ìµœì‹  ë°©ì‹ (ChatMessageHistory)
from langchain_core.chat_history import InMemoryChatMessageHistory
# [LangChain ì‚¬ìš©] ì½œë°± í•¸ë“¤ëŸ¬ - ì§„í–‰ë¥  ì¶”ì 
from langchain_core.callbacks import BaseCallbackHandler

# =============================================================================
# [ë¯¸ì‚¬ìš©] ì•„ë˜ëŠ” SSL ë¬¸ì œë¡œ ì§ì ‘ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ
# from langchain_google_genai import ChatGoogleGenerativeAI
# llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", transport="rest")
# â†’ SSLError: certificate verify failed
# =============================================================================


class GeminiClientError(Exception):
    """Gemini API ì˜¤ë¥˜"""
    def __init__(self, error_type: str, message: str, status_code: int = 0, raw_output: Optional[str] = None):
        super().__init__(message)
        self.error_type = error_type
        self.message = message
        self.status_code = status_code
        self.raw_output = raw_output


# =============================================================================
# LangChain ì½œë°± í•¸ë“¤ëŸ¬ (ì§„í–‰ë¥  ì¶”ì ìš©)
# =============================================================================
class ProgressCallbackHandler(BaseCallbackHandler):
    """
    LangChain ì½œë°± í•¸ë“¤ëŸ¬ - ì§„í–‰ë¥  ì¶”ì 
    
    [LangChain ë°©ì‹]
    - on_llm_start, on_llm_end ë“± ì´ë²¤íŠ¸ ì½œë°±
    - BaseCallbackHandler ìƒì†
    
    [ê¸°ì¡´ REST API ë°©ì‹]
    - ì§ì ‘ progress_callback í•¨ìˆ˜ í˜¸ì¶œ
    - if progress_callback: await progress_callback(step, message)
    """
    
    def __init__(self, progress_callback: Optional[Callable] = None):
        self.progress_callback = progress_callback
        self.current_step = 0
    
    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs):
        """LLM í˜¸ì¶œ ì‹œì‘ ì‹œ"""
        logger.info(f"ğŸ”„ LangChain LLM Start: {len(prompts)} prompts")
    
    def on_llm_end(self, response, **kwargs):
        """LLM í˜¸ì¶œ ì™„ë£Œ ì‹œ"""
        logger.info(f"âœ… LangChain LLM End")
    
    def on_llm_error(self, error: Exception, **kwargs):
        """LLM ì˜¤ë¥˜ ë°œìƒ ì‹œ"""
        logger.error(f"âŒ LangChain LLM Error: {error}")


# =============================================================================
# LangChain í˜¸í™˜ ChatSession
# =============================================================================
class ChatSession:
    """
    LangChain í˜¸í™˜ ChatSession
    
    [LangChain ë°©ì‹ - ì¸í„°í˜ì´ìŠ¤]
    - HumanMessage, AIMessage íƒ€ì… ì‚¬ìš©
    - ConversationBufferMemoryë¡œ íˆìŠ¤í† ë¦¬ ê´€ë¦¬
    
    [REST API ë°©ì‹ - ì‹¤ì œ í˜¸ì¶œ]
    - requests.post(url, json=payload, verify=False)
    - Google Generative AI REST API ì§ì ‘ í˜¸ì¶œ
    """
    
    def __init__(self, client: 'GeminiClient', system_prompt: Optional[str] = None):
        self.client = client
        self.system_prompt = system_prompt
        
        # [LangChain] InMemoryChatMessageHistory ì‚¬ìš© (ìµœì‹  ë°©ì‹)
        # ê¸°ì¡´: ConversationBufferMemory (deprecated)
        # ë³€ê²½: InMemoryChatMessageHistory - langchain_core.chat_history
        self.message_history = InMemoryChatMessageHistory()
        
        # [REST APIìš©] ì§ì ‘ ê´€ë¦¬í•˜ëŠ” íˆìŠ¤í† ë¦¬
        # Google API í˜•ì‹: [{"role": "user/model", "parts": [{"text": "..."}]}]
        self._rest_history: List[Dict[str, Any]] = []
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ ì´ˆê¸° ì»¨í…ìŠ¤íŠ¸ë¡œ ì¶”ê°€
        if system_prompt:
            # [LangChain] ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì €ì¥
            initial_input = system_prompt + "\n\ní”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."
            initial_output = "ë„¤, ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. React í”„ë¡œí† íƒ€ì… ìƒì„±ì„ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤."
            self.message_history.add_user_message(initial_input)
            self.message_history.add_ai_message(initial_output)
            # [REST API] íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self._rest_history.append({
                "role": "user",
                "parts": [{"text": system_prompt + "\n\ní”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."}]
            })
            self._rest_history.append({
                "role": "model",
                "parts": [{"text": "ë„¤, ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. React í”„ë¡œí† íƒ€ì… ìƒì„±ì„ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤."}]
            })
    
    def send_message(
        self,
        prompt: str,
        temperature: float = 0.2,
        max_output_tokens: int = 8192
    ) -> Dict[str, Any]:
        """
        ë©”ì‹œì§€ ì „ì†¡ (ë™ê¸°)
        
        [LangChain ì¸í„°í˜ì´ìŠ¤]
        - HumanMessage, AIMessage ê°ì²´ ìƒì„±
        - ConversationBufferMemoryì— ì €ì¥
        
        [REST API í˜¸ì¶œ]
        - requests.post(url, json=payload, verify=False)
        """
        try:
            # [REST API] ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
            self._rest_history.append({
                "role": "user",
                "parts": [{"text": prompt}]
            })
            
            # [REST API] ì§ì ‘ í˜¸ì¶œ
            response = self.client._call_api(
                contents=self._rest_history,
                temperature=temperature,
                max_output_tokens=max_output_tokens
            )
            
            text = response.get("text", "")
            finish_reason = response.get("finish_reason", 1)
            
            # [REST API] ì‘ë‹µ íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self._rest_history.append({
                "role": "model",
                "parts": [{"text": text}]
            })
            
            # [LangChain] ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì— ì €ì¥ (ìµœì‹  ë°©ì‹)
            self.message_history.add_user_message(prompt)
            self.message_history.add_ai_message(text)
            
            is_truncated = finish_reason == 2
            
            return {
                "text": text,
                "finish_reason": finish_reason,
                "is_truncated": is_truncated
            }
            
        except Exception as e:
            logger.error(f"âŒ ChatSession.send_message failed: {e}")
            raise GeminiClientError("api_error", str(e))
    
    async def send_message_async(
        self,
        prompt: str,
        temperature: float = 0.2,
        max_output_tokens: int = 8192
    ) -> Dict[str, Any]:
        """
        ë©”ì‹œì§€ ì „ì†¡ (ë¹„ë™ê¸°)
        
        [ë¹„ë™ê¸° ì²˜ë¦¬]
        - asyncio.to_thread()ë¡œ ë™ê¸° ë©”ì„œë“œ ë˜í•‘
        
        [ì°¸ê³ ] LangChain SDK ì§ì ‘ ì‚¬ìš© ì‹œ ainvoke() ì§€ì›
        - response = await llm.ainvoke(messages)
        """
        return await asyncio.to_thread(
            self.send_message,
            prompt,
            temperature,
            max_output_tokens
        )
    
    def get_langchain_messages(self) -> List[BaseMessage]:
        """
        [LangChain] ë©”ì‹œì§€ íˆìŠ¤í† ë¦¬ì—ì„œ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        
        ë°˜í™˜ í˜•ì‹:
        - List[HumanMessage | AIMessage]
        
        ê¸°ì¡´ ë°©ì‹: self.memory.load_memory_variables({}).get("chat_history", [])
        ìµœì‹  ë°©ì‹: self.message_history.messages
        """
        return self.message_history.messages


# =============================================================================
# ë©”ì¸ GeminiClient í´ë˜ìŠ¤
# =============================================================================
class GeminiClient:
    """
    LangChain í˜¸í™˜ ì¸í„°í˜ì´ìŠ¤ + REST API ì§ì ‘ í˜¸ì¶œ
    
    [LangChain ì¸í„°í˜ì´ìŠ¤]
    - Message ê°ì²´ (HumanMessage, AIMessage, SystemMessage)
    - ConversationBufferMemory
    - ProgressCallbackHandler
    
    [REST API ì§ì ‘ í˜¸ì¶œ]
    - requests.post(url, verify=False)
    - SSL ì¸ì¦ì„œ ê²€ì¦ ìš°íšŒ
    
    [ì™œ ì´ë ‡ê²Œ êµ¬í˜„í–ˆë‚˜?]
    - LangChain SDKì˜ ChatGoogleGenerativeAIëŠ” SSL ìš°íšŒ ì˜µì…˜ì´ ì—†ìŒ
    - google-generativeai ë‚´ë¶€ HTTP í´ë¼ì´ì–¸íŠ¸ëŠ” verify=False ë¯¸ì§€ì›
    - ìì²´ ì„œëª… ì¸ì¦ì„œ í™˜ê²½ì—ì„œ SSL ì˜¤ë¥˜ ë°œìƒ
    - í•´ê²°: LangChain ì¸í„°í˜ì´ìŠ¤ + requests(verify=False) ì¡°í•©
    """
    
    # Google Generative AI REST API ê¸°ë³¸ URL
    BASE_URL = "https://generativelanguage.googleapis.com/v1beta"
    
    def __init__(self, api_key: Optional[str] = None, model_name: Optional[str] = None):
        self.api_key = api_key or os.getenv("GOOGLE_API_KEY")
        if not self.api_key:
            raise ValueError("GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        self.model_name = model_name or os.getenv("GEMINI_MODEL", "gemini-2.5-flash")
        if self.model_name.endswith("-latest"):
            self.model_name = self.model_name[:-7]
        
        # ì„¤ì •
        self.timeout = 180
        self.max_retries = 3
        self.retry_delay = 60
        self.max_quota_retries = 10
        self.max_continuation_attempts = 3
        
        logger.info(f"GeminiClient initialized: model={self.model_name}")
    
    # =========================================================================
    # REST API ì§ì ‘ í˜¸ì¶œ (í•µì‹¬ ë©”ì„œë“œ)
    # =========================================================================
    
    def _build_url(self, endpoint: str) -> str:
        """API URL ìƒì„±"""
        return f"{self.BASE_URL}/models/{self.model_name}:{endpoint}?key={self.api_key}"
    
    def _call_api(
        self,
        contents: List[Dict[str, Any]],
        temperature: float = 0.2,
        max_output_tokens: int = 8192,
        top_p: float = 0.95,
        top_k: int = 40,
        timeout: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        REST API ì§ì ‘ í˜¸ì¶œ
        
        [ìš”ì²­ í˜•ì‹]
        POST https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent
        {
            "contents": [{"role": "user", "parts": [{"text": "..."}]}],
            "generationConfig": {...}
        }
        
        [SSL ìš°íšŒ]
        requests.post(..., verify=False)
        """
        url = self._build_url("generateContent")
        
        payload = {
            "contents": contents,
            "generationConfig": {
                "temperature": temperature,
                "maxOutputTokens": max_output_tokens,
                "topP": top_p,
                "topK": top_k
            }
        }
        
        try:
            response = requests.post(
                url,
                json=payload,
                timeout=timeout or self.timeout,
                verify=False  # SSL ì¸ì¦ì„œ ê²€ì¦ ë¹„í™œì„±í™”
            )
            
            if response.status_code == 429:
                raise GeminiClientError("quota_exceeded", "API í• ë‹¹ëŸ‰ ì´ˆê³¼", 429)
            
            if response.status_code != 200:
                raise GeminiClientError(
                    "api_error",
                    f"API ì˜¤ë¥˜: {response.status_code} - {response.text}",
                    response.status_code
                )
            
            data = response.json()
            
            # ì‘ë‹µ íŒŒì‹±
            candidates = data.get("candidates", [])
            if not candidates:
                raise GeminiClientError("empty_response", "ë¹ˆ ì‘ë‹µ")
            
            candidate = candidates[0]
            content = candidate.get("content", {})
            parts = content.get("parts", [])
            
            text = ""
            for part in parts:
                if "text" in part:
                    text += part["text"]
            
            # finish_reason í™•ì¸
            finish_reason_str = candidate.get("finishReason", "STOP")
            finish_reason = 1  # STOP
            if finish_reason_str == "MAX_TOKENS":
                finish_reason = 2
            elif finish_reason_str == "SAFETY":
                finish_reason = 3
            
            return {
                "text": text,
                "finish_reason": finish_reason,
                "raw": data
            }
            
        except requests.exceptions.Timeout:
            raise GeminiClientError("timeout", "API ìš”ì²­ ì‹œê°„ ì´ˆê³¼")
        except requests.exceptions.RequestException as e:
            raise GeminiClientError("network_error", f"ë„¤íŠ¸ì›Œí¬ ì˜¤ë¥˜: {str(e)}")
    
    # =========================================================================
    # ê¸°ë³¸ ìƒì„± ë©”ì„œë“œ
    # =========================================================================
    
    def generate_content(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.2,
        max_output_tokens: int = 8192,
        top_p: float = 0.95,
        top_k: int = 40,
        timeout: Optional[int] = None
    ) -> str:
        """
        ì»¨í…ì¸  ìƒì„± (ë™ê¸°)
        
        [LangChain í˜¸í™˜]
        - SystemMessage, HumanMessage ê°œë… ì§€ì›
        
        [REST API í˜¸ì¶œ]
        - ë‚´ë¶€ì ìœ¼ë¡œ _call_api() ì‚¬ìš©
        """
        contents = []
        
        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ìˆìœ¼ë©´ ì²« ë²ˆì§¸ user ë©”ì‹œì§€ë¡œ ì¶”ê°€
        # (Google APIëŠ” ë³„ë„ system role ë¯¸ì§€ì›)
        if system_prompt:
            contents.append({
                "role": "user",
                "parts": [{"text": f"[System Instruction]\n{system_prompt}"}]
            })
            contents.append({
                "role": "model",
                "parts": [{"text": "ì§€ì‹œì‚¬í•­ì„ ì´í•´í–ˆìŠµë‹ˆë‹¤. ìš”ì²­ì— ë”°ë¼ ì§„í–‰í•˜ê² ìŠµë‹ˆë‹¤."}]
            })
        
        contents.append({
            "role": "user",
            "parts": [{"text": prompt}]
        })
        
        response = self._call_api(
            contents=contents,
            temperature=temperature,
            max_output_tokens=max_output_tokens,
            top_p=top_p,
            top_k=top_k,
            timeout=timeout
        )
        
        return response.get("text", "")
    
    async def generate_content_async(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: float = 0.2,
        max_output_tokens: int = 8192,
        top_p: float = 0.95,
        top_k: int = 40,
        timeout: Optional[int] = None
    ) -> str:
        """
        ì»¨í…ì¸  ìƒì„± (ë¹„ë™ê¸°)
        
        [ë¹„ë™ê¸° ì²˜ë¦¬]
        - asyncio.to_thread()ë¡œ ë™ê¸° ë©”ì„œë“œ ë˜í•‘
        """
        return await asyncio.to_thread(
            self.generate_content,
            prompt,
            system_prompt,
            temperature,
            max_output_tokens,
            top_p,
            top_k,
            timeout
        )
    
    # =========================================================================
    # ChatSession ê´€ë ¨
    # =========================================================================
    
    def start_chat(self, history: Optional[List[Dict[str, Any]]] = None) -> ChatSession:
        """ìƒˆ ChatSession ì‹œì‘"""
        return ChatSession(self)
    
    def start_chat_with_system_prompt(self, system_prompt: str) -> ChatSession:
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¡œ ChatSession ì‹œì‘"""
        return ChatSession(self, system_prompt)
    
    # =========================================================================
    # ì¬ì‹œë„ ë¡œì§
    # =========================================================================
    
    async def send_chat_with_retry(
        self,
        chat_session: ChatSession,
        prompt: str,
        operation_name: str = "Chat",
        temperature: float = 0.2,
        max_output_tokens: int = 8192
    ) -> Dict[str, Any]:
        """
        ChatSession ë©”ì‹œì§€ ì „ì†¡ (í• ë‹¹ëŸ‰ ì¬ì‹œë„ í¬í•¨)
        """
        for attempt in range(self.max_quota_retries):
            try:
                response = await chat_session.send_message_async(
                    prompt=prompt,
                    temperature=temperature,
                    max_output_tokens=max_output_tokens
                )
                return response
                
            except GeminiClientError as e:
                if e.error_type == "quota_exceeded":
                    if attempt < self.max_quota_retries - 1:
                        wait_time = self.retry_delay * (attempt + 1)
                        logger.warning(
                            f"â³ {operation_name}: í• ë‹¹ëŸ‰ ì´ˆê³¼, {wait_time}ì´ˆ ëŒ€ê¸° "
                            f"(ì¬ì‹œë„ {attempt + 1}/{self.max_quota_retries})"
                        )
                        await asyncio.sleep(wait_time)
                        continue
                raise
            except Exception as e:
                if "429" in str(e) or "quota" in str(e).lower():
                    if attempt < self.max_quota_retries - 1:
                        wait_time = self.retry_delay * (attempt + 1)
                        logger.warning(f"â³ {operation_name}: ì¬ì‹œë„ ëŒ€ê¸° {wait_time}ì´ˆ")
                        await asyncio.sleep(wait_time)
                        continue
                raise
        
        raise GeminiClientError("quota_exceeded", f"{operation_name}: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
    
    # =========================================================================
    # ì½”ë“œ ì—°ì† ìƒì„± (ëŠê¹€ ì²˜ë¦¬)
    # =========================================================================
    
    async def continue_truncated_code(
        self,
        chat_session: ChatSession,
        initial_response: Dict[str, Any],
        operation_name: str = "Code"
    ) -> str:
        """
        ëŠê¸´ ì½”ë“œ ì—°ì† ìƒì„±
        
        ì‘ë‹µì´ MAX_TOKENSë¡œ ëŠê¸´ ê²½ìš° ì´ì–´ì„œ ìƒì„±
        """
        result = initial_response.get("text", "")
        is_truncated = initial_response.get("is_truncated", False)
        
        for attempt in range(self.max_continuation_attempts):
            if not is_truncated:
                break
            
            logger.info(f"ğŸ”„ {operation_name}: ì½”ë“œ ì—°ì† ìƒì„± {attempt + 1}/{self.max_continuation_attempts}")
            
            continuation_prompt = (
                "ì½”ë“œê°€ ëŠê²¼ìŠµë‹ˆë‹¤. ì´ì „ ì‘ë‹µì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ë¶€í„° ì´ì–´ì„œ "
                "ë‚¨ì€ ì½”ë“œë¥¼ ì™„ì„±í•´ì£¼ì„¸ìš”. ì¤‘ë³µ ì—†ì´ ì´ì–´ì„œ ì‘ì„±í•´ì£¼ì„¸ìš”."
            )
            
            try:
                response = await self.send_chat_with_retry(
                    chat_session=chat_session,
                    prompt=continuation_prompt,
                    operation_name=f"{operation_name}-continuation",
                    max_output_tokens=8192
                )
                
                continuation_text = response.get("text", "")
                result += "\n" + continuation_text
                is_truncated = response.get("is_truncated", False)
                
            except Exception as e:
                logger.error(f"âŒ ì—°ì† ìƒì„± ì‹¤íŒ¨: {e}")
                break
        
        return result
    
    # =========================================================================
    # í”„ë¡œí† íƒ€ì… ìƒì„± (4ë‹¨ê³„)
    # =========================================================================
    
    async def generate_prototype(
        self,
        full_prompt: str,
        system_prompt: str,
        progress_callback: Optional[Callable[[str, int, int], Awaitable[None]]] = None
    ) -> Dict[str, Any]:
        """
        í”„ë¡œí† íƒ€ì… ìƒì„± (4ë‹¨ê³„)
        
        1ë‹¨ê³„: ê¸°ë³¸ êµ¬ì¡° ìƒì„±
        2ë‹¨ê³„: ê¸°ëŠ¥ êµ¬í˜„
        3ë‹¨ê³„: ìŠ¤íƒ€ì¼ë§
        4ë‹¨ê³„: ìµœì í™” ë° ë§ˆë¬´ë¦¬
        """
        logger.info("ğŸš€ í”„ë¡œí† íƒ€ì… ìƒì„± ì‹œì‘ (4ë‹¨ê³„)")
        
        # ì½œë°± í•¸ë“¤ëŸ¬ ìƒì„±
        callback_handler = ProgressCallbackHandler(progress_callback)
        
        # ChatSession ì‹œì‘
        chat_session = self.start_chat_with_system_prompt(system_prompt)
        
        generated_code = ""
        stage_prompts = self._create_stage_prompts(full_prompt)
        
        for stage, (stage_name, stage_prompt) in enumerate(stage_prompts, 1):
            logger.info(f"ğŸ“‹ Stage {stage}/4: {stage_name}")
            
            if progress_callback:
                await progress_callback(f"Stage {stage}/4: {stage_name}", stage, 4)
            
            try:
                response = await self.send_chat_with_retry(
                    chat_session=chat_session,
                    prompt=stage_prompt,
                    operation_name=f"Stage-{stage}",
                    temperature=0.2,
                    max_output_tokens=8192
                )
                
                # ëŠê¹€ ì²˜ë¦¬
                if response.get("is_truncated"):
                    full_response = await self.continue_truncated_code(
                        chat_session=chat_session,
                        initial_response=response,
                        operation_name=f"Stage-{stage}"
                    )
                else:
                    full_response = response.get("text", "")
                
                # ì½”ë“œ ì¶”ì¶œ
                stage_code = self._extract_code_from_response(full_response)
                
                if stage_code:
                    generated_code = stage_code
                    logger.info(f"âœ… Stage {stage} ì™„ë£Œ: {len(stage_code)} chars")
                
            except Exception as e:
                logger.error(f"âŒ Stage {stage} ì‹¤íŒ¨: {e}")
                if stage == 1:
                    raise
                # Stage 1 ì´í›„ ì‹¤íŒ¨ëŠ” ì´ì „ ê²°ê³¼ ì‚¬ìš©
                break
        
        if not generated_code:
            raise GeminiClientError("generation_failed", "í”„ë¡œí† íƒ€ì… ìƒì„± ì‹¤íŒ¨")
        
        # ìµœì¢… ì •ë¦¬
        final_code = self._clean_generated_code(generated_code)
        
        return {
            "code": final_code,
            "language": "tsx",
            "stages_completed": min(len(stage_prompts), 4)
        }
    
    def _create_stage_prompts(self, full_prompt: str) -> List[Tuple[str, str]]:
        """4ë‹¨ê³„ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
        return [
            ("ê¸°ë³¸ êµ¬ì¡° ìƒì„±", f"""
{full_prompt}

[Stage 1 - ê¸°ë³¸ êµ¬ì¡°]
ìœ„ ìš”êµ¬ì‚¬í•­ì„ ë¶„ì„í•˜ê³  ê¸°ë³¸ ì»´í¬ë„ŒíŠ¸ êµ¬ì¡°ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.
- ë©”ì¸ ì»´í¬ë„ŒíŠ¸ êµ¬ì¡°
- í•„ìš”í•œ ìƒíƒœ ì •ì˜
- ê¸°ë³¸ ë ˆì´ì•„ì›ƒ
"""),
            ("ê¸°ëŠ¥ êµ¬í˜„", """
[Stage 2 - ê¸°ëŠ¥ êµ¬í˜„]
ì´ì „ ë‹¨ê³„ì˜ êµ¬ì¡°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•µì‹¬ ê¸°ëŠ¥ì„ êµ¬í˜„í•´ì£¼ì„¸ìš”.
- ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
- ë°ì´í„° ì²˜ë¦¬ ë¡œì§
- ìƒíƒœ ê´€ë¦¬ ë¡œì§
ì™„ì „í•œ ì½”ë“œë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
"""),
            ("ìŠ¤íƒ€ì¼ë§", """
[Stage 3 - ìŠ¤íƒ€ì¼ë§]
Tailwind CSSë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íƒ€ì¼ì„ ì ìš©í•´ì£¼ì„¸ìš”.
- ë°˜ì‘í˜• ë””ìì¸
- ì‹œê°ì  ê°œì„ 
- UX ìµœì í™”
ì™„ì „í•œ ì½”ë“œë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
"""),
            ("ìµœì í™” ë° ë§ˆë¬´ë¦¬", """
[Stage 4 - ìµœì í™” ë° ë§ˆë¬´ë¦¬]
ìµœì¢… ì½”ë“œë¥¼ ì™„ì„±í•´ì£¼ì„¸ìš”.
- ì½”ë“œ ì •ë¦¬
- ì£¼ì„ ì¶”ê°€
- ì—ëŸ¬ ì²˜ë¦¬
- TypeScript íƒ€ì… ì™„ì„±

ìµœì¢… ì™„ì„±ëœ ì „ì²´ ì½”ë“œë¥¼ ì œê³µí•´ì£¼ì„¸ìš”.
""")
        ]
    
    def _extract_code_from_response(self, response_text: str) -> str:
        """ì‘ë‹µì—ì„œ ì½”ë“œ ì¶”ì¶œ"""
        patterns = [
            r'```tsx\s*([\s\S]*?)```',
            r'```typescript\s*([\s\S]*?)```',
            r'```jsx\s*([\s\S]*?)```',
            r'```javascript\s*([\s\S]*?)```',
            r'```react\s*([\s\S]*?)```',
            r'```\s*([\s\S]*?)```',
        ]
        
        for pattern in patterns:
            matches = re.findall(pattern, response_text, re.IGNORECASE)
            if matches:
                # ê°€ì¥ ê¸´ ì½”ë“œ ë¸”ë¡ ì„ íƒ
                return max(matches, key=len).strip()
        
        # ì½”ë“œ ë¸”ë¡ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜
        return response_text.strip()
    
    def _clean_generated_code(self, code: str) -> str:
        """ìƒì„±ëœ ì½”ë“œ ì •ë¦¬"""
        # ë§ˆì»¤ ì œê±°
        code = re.sub(r'\[Stage \d.*?\]', '', code)
        code = re.sub(r'// Stage \d.*?\n', '', code)
        
        # ì¤‘ë³µ import ì œê±°
        lines = code.split('\n')
        seen_imports = set()
        clean_lines = []
        
        for line in lines:
            if line.strip().startswith('import '):
                if line.strip() not in seen_imports:
                    seen_imports.add(line.strip())
                    clean_lines.append(line)
            else:
                clean_lines.append(line)
        
        return '\n'.join(clean_lines).strip()


# =============================================================================
# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
# =============================================================================
_gemini_client: Optional[GeminiClient] = None


def get_gemini_client() -> GeminiClient:
    """GeminiClient ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _gemini_client
    if _gemini_client is None:
        _gemini_client = GeminiClient()
    return _gemini_client


def reset_gemini_client():
    """í´ë¼ì´ì–¸íŠ¸ ë¦¬ì…‹ (í…ŒìŠ¤íŠ¸ìš©)"""
    global _gemini_client
    _gemini_client = None
