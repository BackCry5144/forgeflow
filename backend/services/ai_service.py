# services/ai_service.py

import os
import logging
import ssl
import asyncio
from typing import Dict, Any, Optional, Callable, Awaitable
from datetime import timedelta
import google.generativeai as genai
from google.api_core import exceptions as google_exceptions

from utils.prompt_templates import (
    SYSTEM_PROMPT,
    get_step_1_prompt,
    get_step_2_prompt,
    get_step_3_prompt,
    get_step_4_prompt,
)
from services.cache_service import get_cache_service

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# SSL ì¸ì¦ì„œ ê²€ì¦ ì™„ì „ ìš°íšŒ ì„¤ì • (ê°œë°œ í™˜ê²½ìš©)
# ============================================================================
os.environ['GRPC_VERBOSITY'] = 'NONE'
os.environ['GRPC_TRACE'] = ''
os.environ['GRPC_ENABLE_FORK_SUPPORT'] = '1'
os.environ['GRPC_SSL_CIPHER_SUITES'] = 'HIGH+ECDSA'
os.environ['GRPC_DEFAULT_SSL_ROOTS_FILE_PATH'] = ''
os.environ['SSL_CERT_FILE'] = ''
os.environ['REQUESTS_CA_BUNDLE'] = ''
os.environ['CURL_CA_BUNDLE'] = ''
os.environ['PYTHONHTTPSVERIFY'] = '0'
ssl._create_default_https_context = ssl._create_unverified_context

try:
    import warnings
    warnings.filterwarnings('ignore', category=DeprecationWarning)
    # gRPC ë‚´ë¶€ ë¡œê±° ë¹„í™œì„±í™”
    grpc_logger = logging.getLogger('grpc')
    grpc_logger.setLevel(logging.CRITICAL)
    grpc_logger.addHandler(logging.NullHandler())
    grpc_logger.propagate = False
except Exception:
    pass
# ============================================================================


class AIServiceError(Exception):
    """Structured exception for AI service failures."""
    def __init__(self, error_type: str, message: str, raw_output: Optional[str] = None):
        super().__init__(message)
        self.error_type = error_type
        self.message = message
        self.raw_output = raw_output


class AIService:
    """Google AI Studio (Gemini) APIë¥¼ ì‚¬ìš©í•œ AI ì„œë¹„ìŠ¤ with Context Caching & ChatSession"""
    
    def __init__(self):
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        genai.configure(api_key=api_key)
        
        self.model_name = os.getenv("GEMINI_MODEL", "gemini-1.5-pro")
        if self.model_name.endswith("-latest"):
            self.model_name = self.model_name[:-7]
            
        self.model = genai.GenerativeModel(self.model_name)
        self.cache_service = get_cache_service()
        
        # ì„¤ì •ê°’
        self.max_continuation_attempts = 3
        self.max_quota_retries = 10
        self.retry_delay_seconds = 60
        self.caching_enabled = self._check_caching_feasibility()
        
        logger.info(f"AI Service initialized: {self.model_name}")
        logger.info(f"Caching: {'Enabled' if self.caching_enabled else 'Disabled'}")
    
    def _check_caching_feasibility(self) -> bool:
        prompt_length = len(SYSTEM_PROMPT)
        if prompt_length < 600:
            return False
        return True

    # -------------------------------------------------------------------------
    # 1. Chat ì „ì†¡ í—¬í¼ (ê¸°ì¡´ _call_api_with_retry ëŒ€ì²´)
    # -------------------------------------------------------------------------
    async def _send_chat_with_retry(self, chat_session, prompt, operation_name="Chat"):
        """ChatSession ë©”ì‹œì§€ ì „ì†¡ ë° í• ë‹¹ëŸ‰ ì¬ì‹œë„ ì²˜ë¦¬"""

        # ì½”ë“œ ìƒì„±ì— ìµœì í™”ëœ ì„¤ì • (ì˜¨ë„ 0.2) - Gemini API ê¶Œì¥ê°’
        generation_config = {
            "temperature": 0.2,        # ì°½ì˜ì„± ë‚®ì¶¤ -> ì§€ì‹œ ì¤€ìˆ˜ ë° ì¼ê´€ì„± í–¥ìƒ
            "max_output_tokens": 8192, # ìµœëŒ€ í† í° ì‚¬ìš©
            "top_p": 0.95,             # (ì„ íƒ) ì¼ë°˜ì ì¸ ì½”ë“œ ìƒì„± ê¶Œì¥ê°’
            "top_k": 40,               # (ì„ íƒ) ì¼ë°˜ì ì¸ ì½”ë“œ ìƒì„± ê¶Œì¥ê°’
        }

        for attempt in range(self.max_quota_retries + 1):
            try:
                response = await chat_session.send_message_async(
                    prompt,
                    generation_config=generation_config
                )
                return response
            except google_exceptions.ResourceExhausted:
                if attempt < self.max_quota_retries:
                    wait_time = self.retry_delay_seconds
                    logger.warning(f"â³ [{operation_name}] Quota exceeded. Retrying in {wait_time}s...")
                    await asyncio.sleep(wait_time)
                else:
                    raise AIServiceError("quota_exceeded", f"Quota exceeded after {self.max_quota_retries} retries.")
            except Exception as e:
                logger.error(f"âŒ [{operation_name}] Failed: {e}")
                raise e
        raise AIServiceError("unknown_error", "Unexpected error in chat retry")

    # -------------------------------------------------------------------------
    # 2. ë‹¨ê³„ë³„ ì´ì–´ë°›ê¸° í—¬í¼ (ê¸°ì¡´ _continue_generation ëŒ€ì²´)
    # -------------------------------------------------------------------------
    async def _handle_step_continuation(self, chat_session, initial_response, step_name):
        """ê° ë‹¨ê³„ ë‚´ì—ì„œ í† í°ì´ ì˜ë ¸ì„ ë•Œ ë¬¸ë§¥ì„ ìœ ì§€í•˜ë©° ì´ì–´ë°›ê¸°"""
        full_text = initial_response.text
        
        if not initial_response.candidates:
             return full_text # í›„ë³´ê°€ ì—†ìœ¼ë©´ ê·¸ëŒ€ë¡œ ë°˜í™˜

        finish_reason = initial_response.candidates[0].finish_reason
        
        attempt = 0
        # 2 = MAX_TOKENS
        while finish_reason == 2 and attempt < self.max_continuation_attempts:
            attempt += 1
            logger.warning(f"âš ï¸ {step_name} truncated (MAX_TOKENS). Continuation #{attempt}...")
            
            # ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•´ ë§ˆì§€ë§‰ 100ìë¥¼ í¬í•¨í•˜ì—¬ ìš”ì²­
            last_chars = full_text.strip()[-100:]
            continuation_prompt = (
                f"âš ï¸ SYSTEM: Previous response was truncated. "
                f"CONTINUE generating the code exactly from where it stopped:\n"
                f"...{last_chars}\n"
                f"(Do NOT repeat the context above. Just output the rest of the code.)"
            )
            
            response = await self._send_chat_with_retry(
                chat_session, 
                continuation_prompt, 
                operation_name=f"{step_name} Continuation #{attempt}"
            )
            
            part_text = response.text
            
            # ë§ˆí¬ë‹¤ìš´ ë¸”ë¡(```) ì œê±° ë¡œì§
            if part_text.strip().startswith("```"):
                lines = part_text.strip().split('\n')
                # ì²« ì¤„ì´ ì–¸ì–´ ì„ ì–¸ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œê±°í•˜ê³  í•©ì¹¨
                part_text = '\n'.join(lines[1:])
            if part_text.strip().endswith("```"):
                part_text = part_text.strip()[:-3]
            
            full_text += part_text
            
            if response.candidates:
                finish_reason = response.candidates[0].finish_reason
            else:
                break
            
        if finish_reason == 2:
            logger.error(f"âŒ {step_name} still truncated after max attempts.")
            
        return full_text

    # -------------------------------------------------------------------------
    # 3. ë©”ì¸ ìƒì„± í•¨ìˆ˜ (4ë‹¨ê³„ ìˆœì°¨ ì‹¤í–‰ + ì§„í–‰ë¥  ì½œë°±)
    # -------------------------------------------------------------------------
    async def generate_prototype(
        self, 
        # prompt: str, # Legacy ì œê±°
        menu_name: str,
        screen_name: str,
        wizard_data: Optional[Dict[str, Any]] = None,
        progress_callback: Optional[Callable[[int, str], Awaitable[None]]] = None
    ) -> Dict[str, str]:
        """Wizard ê¸°ë°˜ 4ë‹¨ê³„ ìˆœì°¨ì  ì½”ë“œ ìƒì„± (ì§„í–‰ë¥  ì½œë°± í¬í•¨)"""
        logger.info(f"ğŸš€ Starting 4-Stage Generation for: {menu_name}/{screen_name}")
        
        if not wizard_data:
            raise AIServiceError("missing_wizard_data", "Wizard data required.")

        # 1. ChatSession ì´ˆê¸°í™”
        chat_session = None
        cached_context = None
        
        if self.caching_enabled:
            cached_context = self.cache_service.get_cached_context(SYSTEM_PROMPT)

        try:
            if cached_context:
                logger.info(f"âœ¨ Using CACHED context: {cached_context['cache_id']}")
                cached_content = genai.caching.CachedContent(name=cached_context['cache_id'])
                model_with_cache = genai.GenerativeModel.from_cached_content(cached_content)
                chat_session = model_with_cache.start_chat()
            else:
                logger.info("ğŸ†• No cache found. Starting fresh chat.")
                chat_session = self.model.start_chat(history=[
                    {"role": "user", "parts": [SYSTEM_PROMPT, "Start project."]},
                    {"role": "model", "parts": ["Ready."]}
                ])
        except Exception as e:
            logger.error(f"âŒ Chat setup failed: {e}")
            # ìµœí›„ì˜ ìˆ˜ë‹¨: ì¼ë°˜ ëª¨ë¸ë¡œ ì‹œì‘
            chat_session = self.model.start_chat(history=[
                 {"role": "user", "parts": [SYSTEM_PROMPT, "Start."]},
                 {"role": "model", "parts": ["OK."]}
            ])

        # 2. 4ë‹¨ê³„ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„ (ì§„í–‰ë¥  ë° ë©”ì‹œì§€ ì •ì˜)
        try:
            steps = [
                # (Step ì´ë¦„, í”„ë¡¬í”„íŠ¸ í•¨ìˆ˜, ì§„í–‰ë¥ (%), ì‚¬ìš©ì í‘œì‹œ ë©”ì‹œì§€)
                ("Step 1 (Utils)", get_step_1_prompt(wizard_data), 40, "ê¸°ì´ˆ ì„¤ì • ë° ìœ í‹¸ë¦¬í‹° ì •ì˜ ì¤‘..."),
                ("Step 2 (State)", get_step_2_prompt(wizard_data), 55, "ìƒíƒœ ê´€ë¦¬ ë° ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ êµ¬í˜„ ì¤‘..."),
                ("Step 3 (UI)", get_step_3_prompt(wizard_data), 75, "ë©”ì¸ í™”ë©´ UI ë Œë”ë§ ì¤‘..."),
                ("Step 4 (Modals)", get_step_4_prompt(wizard_data), 90, "ëª¨ë‹¬ íŒì—… ë° ìµœì¢… ì™„ì„± ì¤‘...")
            ]
        except Exception as e:
            raise AIServiceError("prompt_error", f"í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± ì‹¤íŒ¨: {e}")

        # í”„ë¡¬í”„íŠ¸ ë¡œê·¸ ëˆ„ì  ë³€ìˆ˜ ì´ˆê¸°í™”
        full_prompt_log = f"=== [SYSTEM PROMPT] ===\n{SYSTEM_PROMPT}\n\n"

        # 3. ìˆœì°¨ì  ì‹¤í–‰
        generated_code_chunks = []
        
        for i, (step_name, step_prompt, progress_percent, user_message) in enumerate(steps):
            logger.info(f"ğŸ”„ Processing {step_name}...")

            # í˜„ì¬ ë‹¨ê³„ì˜ í”„ë¡¬í”„íŠ¸ë¥¼ ë¡œê·¸ì— ëˆ„ì 
            full_prompt_log += f"=== [{step_name}] ===\n{step_prompt}\n\n"

            # ğŸ”¥ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (ì½œë°± í˜¸ì¶œ)
            if progress_callback:
                try:
                    await progress_callback(progress_percent, user_message)
                except Exception as e:
                    logger.warning(f"âš ï¸ Progress callback failed: {e}")

            # ë©”ì‹œì§€ ì „ì†¡
            response = await self._send_chat_with_retry(chat_session, step_prompt, operation_name=step_name)
            
            # ê²°ê³¼ ì²˜ë¦¬ (Continuation í¬í•¨)
            full_step_text = await self._handle_step_continuation(chat_session, response, step_name)
            
            # ë§ˆí¬ë‹¤ìš´ ì œê±° ë° ì •ì œ
            cleaned_text = full_step_text.strip()
            if cleaned_text.startswith("```"):
                lines = cleaned_text.split('\n')
                # ì²« ì¤„ì´ ì–¸ì–´ ì„ ì–¸(jsx, javascript)ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì œê±°
                if len(lines) > 0 and (lines[0].startswith("```jsx") or lines[0].startswith("```javascript") or lines[0].strip() == "```"):
                     cleaned_text = '\n'.join(lines[1:])
                else:
                     cleaned_text = '\n'.join(lines[1:]) # ì•ˆì „í•˜ê²Œ ì²«ì¤„ ì œê±°
            if cleaned_text.endswith("```"):
                cleaned_text = cleaned_text[:-3]
            
            generated_code_chunks.append(cleaned_text.strip())
            logger.info(f"âœ… {step_name} Completed. Length: {len(cleaned_text)}")

        # 4. ê²°ê³¼ ë³‘í•© ë° í›„ì²˜ë¦¬
        final_code = "\n\n".join(generated_code_chunks)
        
        # í›„ì²˜ë¦¬: ì¤‘ë³µëœ React Hook ì„ ì–¸ ì œê±°
        import re
        hook_declarations = re.findall(r"const\s*{\s*useState.*}\s*=\s*React;", final_code)
        if len(hook_declarations) > 1:
            logger.warning(f"Found {len(hook_declarations)} duplicate hook declarations. Cleaning up...")
            # ì²« ë²ˆì§¸ ì„ ì–¸ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ëª¨ë‘ ì œê±°
            first_declaration = hook_declarations[0]
            # ëª¨ë“  ì„ ì–¸ì„ ì„ì‹œ í”Œë ˆì´ìŠ¤í™€ë”ë¡œ ë³€ê²½
            final_code = final_code.replace(first_declaration, "__HOOK_DECLARATION_PLACEHOLDER__")
            # ë‚˜ë¨¸ì§€ ì¤‘ë³µ ì„ ì–¸ë“¤ì„ ì œê±°
            for declaration in hook_declarations:
                final_code = final_code.replace(declaration, "")
            # í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ë‹¤ì‹œ ì›ë³¸ ì„ ì–¸ìœ¼ë¡œ ë³µì›
            final_code = final_code.replace("__HOOK_DECLARATION_PLACEHOLDER__", first_declaration)
            logger.info("âœ… Cleanup complete. Only one hook declaration remains.")

        # Sanity Check
        if "export default function" not in final_code:
            logger.warning("âš ï¸ Final code might be incomplete (missing export default)")

        return {
            "prototype_html": final_code,
            "final_prompt": "4-Step Chat Process",
            "full_prompt": full_prompt_log         # ëˆ„ì ëœ ì „ì²´ í”„ë¡¬í”„íŠ¸ ë°˜í™˜
        }

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_ai_service_instance: Optional[AIService] = None

def get_ai_service() -> AIService:
    """AI ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _ai_service_instance
    if _ai_service_instance is None:
        _ai_service_instance = AIService()
    return _ai_service_instance