# services/ai_service.py

import os
import logging
import ssl
import asyncio
import re
from typing import Dict, Any, Optional, Callable, Awaitable
from datetime import timedelta
import google.generativeai as genai
from google.api_core import exceptions as google_exceptions

from utils.prompt_templates import (
    SYSTEM_PROMPT,
    get_step_1_prompt,
    get_step_2_prompt,
    get_step_3_prompt,
    get_step_4_prompt,
)
from services.cache_service import get_cache_service

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# SSL ì¸ì¦ì„œ ê²€ì¦ ì™„ì „ ìš°íšŒ ì„¤ì • (ê°œë°œ í™˜ê²½ìš©)
# ============================================================================
os.environ['GRPC_VERBOSITY'] = 'NONE'
os.environ['GRPC_TRACE'] = ''
os.environ['GRPC_ENABLE_FORK_SUPPORT'] = '1'
os.environ['GRPC_SSL_CIPHER_SUITES'] = 'HIGH+ECDSA'
os.environ['GRPC_DEFAULT_SSL_ROOTS_FILE_PATH'] = ''
os.environ['SSL_CERT_FILE'] = ''
os.environ['REQUESTS_CA_BUNDLE'] = ''
os.environ['CURL_CA_BUNDLE'] = ''
os.environ['PYTHONHTTPSVERIFY'] = '0'
ssl._create_default_https_context = ssl._create_unverified_context

try:
    import warnings
    warnings.filterwarnings('ignore', category=DeprecationWarning)
    grpc_logger = logging.getLogger('grpc')
    grpc_logger.setLevel(logging.CRITICAL)
    grpc_logger.addHandler(logging.NullHandler())
    grpc_logger.propagate = False
except Exception:
    pass
# ============================================================================

# Context Caching ìµœì†Œ í† í° ìš”êµ¬ì‚¬í•­ (Gemini 1.5 Pro ê¸°ì¤€: 32,768 í† í°)
# ëŒ€ëµ 4ì = 1í† í°ìœ¼ë¡œ ê³„ì‚°í•˜ë©´ ì•½ 130,000ì ì´ìƒ í•„ìš”
# í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ë” ì ì€ ì–‘ìœ¼ë¡œë„ ì‹œë„ ê°€ëŠ¥ (APIê°€ ê±°ë¶€í•˜ë©´ fallback)
MIN_TOKENS_FOR_CACHING = 32768
MIN_CHARS_FOR_CACHING = MIN_TOKENS_FOR_CACHING * 4  # ~130,000ì

class AIServiceError(Exception):
    """Structured exception for AI service failures."""
    def __init__(self, error_type: str, message: str, raw_output: Optional[str] = None):
        super().__init__(message)
        self.error_type = error_type
        self.message = message
        self.raw_output = raw_output


class AIService:
    """Google AI Studio (Gemini) APIë¥¼ ì‚¬ìš©í•œ AI ì„œë¹„ìŠ¤ with Context Caching & ChatSession"""
    
    def __init__(self):
        api_key = os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("GOOGLE_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        genai.configure(api_key=api_key)
        
        self.model_name = os.getenv("GEMINI_MODEL", "gemini-1.5-pro")
        if self.model_name.endswith("-latest"):
            self.model_name = self.model_name[:-7]
            
        self.model = genai.GenerativeModel(self.model_name)
        self.cache_service = get_cache_service()
        
        # ì„¤ì •ê°’
        self.max_continuation_attempts = 3
        self.max_quota_retries = 10
        self.retry_delay_seconds = 60
        self.cache_ttl_hours = 1  # Context Cache TTL (1ì‹œê°„)
        self.caching_enabled = self._check_caching_feasibility()
        
        logger.info(f"AI Service initialized: {self.model_name}")
        logger.info(f"Context Caching: {'Enabled' if self.caching_enabled else 'Disabled'} (SYSTEM_PROMPT: {len(SYSTEM_PROMPT)} chars)")
    
    def _check_caching_feasibility(self) -> bool:
        """Context Caching ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ (ìµœì†Œ í† í° ìš”êµ¬ì‚¬í•­)"""
        prompt_length = len(SYSTEM_PROMPT)
        # Gemini Context Cachingì€ ìµœì†Œ 32,768 í† í° í•„ìš”
        # í˜„ì¬ SYSTEM_PROMPTê°€ ì´ë³´ë‹¤ ì‘ìœ¼ë©´ ì¼ë°˜ ChatSession ì‚¬ìš©
        if prompt_length < 600:  # ë„ˆë¬´ ì‘ì€ ê²½ìš° ë¹„í™œì„±í™”
            logger.info(f"âš ï¸ SYSTEM_PROMPT too short for caching ({prompt_length} chars)")
            return False
        return True

    # -------------------------------------------------------------------------
    # Context Caching í—¬í¼
    # -------------------------------------------------------------------------
    def _create_context_cache(self) -> Optional[str]:
        """
        Google Gemini Context Cache ìƒì„± ë° Redisì— ì €ì¥
        
        Returns:
            str: ìºì‹œ ID (ì„±ê³µ ì‹œ) ë˜ëŠ” None (ì‹¤íŒ¨ ì‹œ)
        """
        try:
            logger.info("ğŸ”„ Creating new Gemini Context Cache...")
            
            # Context Cache ìƒì„± (SYSTEM_PROMPTë¥¼ ìºì‹±)
            cached_content = genai.caching.CachedContent.create(
                model=f"models/{self.model_name}",
                display_name="forgeflow-system-prompt",
                contents=[
                    {
                        "role": "user",
                        "parts": [{"text": SYSTEM_PROMPT}]
                    },
                    {
                        "role": "model", 
                        "parts": [{"text": "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì´í•´í–ˆìŠµë‹ˆë‹¤. React í”„ë¡œí† íƒ€ì… ìƒì„±ì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤."}]
                    }
                ],
                ttl=timedelta(hours=self.cache_ttl_hours)
            )
            
            cache_id = cached_content.name
            logger.info(f"âœ… Context Cache created: {cache_id}")
            
            # Redisì— ìºì‹œ ID ì €ì¥
            if self.cache_service.is_available():
                self.cache_service.set_cached_context(
                    system_prompt=SYSTEM_PROMPT,
                    cache_id=cache_id,
                    ttl_hours=self.cache_ttl_hours
                )
            
            return cache_id
            
        except google_exceptions.InvalidArgument as e:
            # í† í° ìˆ˜ê°€ ë¶€ì¡±í•œ ê²½ìš° (ìµœì†Œ 32,768 í† í° í•„ìš”)
            logger.warning(f"âš ï¸ Context Cache creation failed (token count too low): {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ Context Cache creation failed: {e}")
            return None

    def _get_or_create_cached_model(self):
        """
        ìºì‹œëœ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸° ë˜ëŠ” ìƒˆë¡œ ìƒì„±
        
        Returns:
            tuple: (model, is_cached)
        """
        if not self.caching_enabled:
            return self.model, False
        
        # 1. Redisì—ì„œ ê¸°ì¡´ ìºì‹œ ì¡°íšŒ
        cached_context = self.cache_service.get_cached_context(SYSTEM_PROMPT)
        
        if cached_context:
            try:
                logger.info(f"âœ¨ Using existing Context Cache: {cached_context['cache_id']}")
                cached_content = genai.caching.CachedContent.get(cached_context['cache_id'])
                model_with_cache = genai.GenerativeModel.from_cached_content(cached_content)
                return model_with_cache, True
            except Exception as e:
                logger.warning(f"âš ï¸ Failed to load cached content: {e}. Creating new cache...")
                # ìºì‹œê°€ ë§Œë£Œë˜ì—ˆê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŒ - Redisì—ì„œ ì‚­ì œ
                self.cache_service.invalidate_cache(SYSTEM_PROMPT)
        
        # 2. ìƒˆ ìºì‹œ ìƒì„± ì‹œë„
        cache_id = self._create_context_cache()
        
        if cache_id:
            try:
                cached_content = genai.caching.CachedContent.get(cache_id)
                model_with_cache = genai.GenerativeModel.from_cached_content(cached_content)
                return model_with_cache, True
            except Exception as e:
                logger.error(f"âŒ Failed to use new cache: {e}")
        
        # 3. ìºì‹± ì‹¤íŒ¨ ì‹œ ì¼ë°˜ ëª¨ë¸ ë°˜í™˜
        logger.info("ğŸ“ Using standard model (no caching)")
        return self.model, False

    # -------------------------------------------------------------------------
    # 1. Chat ì „ì†¡ í—¬í¼ (ChatSession ê¸°ë°˜)
    # -------------------------------------------------------------------------
    async def _send_chat_with_retry(self, chat_session, prompt, operation_name="Chat"):
        """ChatSession ë©”ì‹œì§€ ì „ì†¡ ë° í• ë‹¹ëŸ‰ ì¬ì‹œë„ ì²˜ë¦¬"""
        
        # ì½”ë“œ ìƒì„±ì— ìµœì í™”ëœ ì„¤ì • (ì˜¨ë„ 0.2)
        generation_config = {
            "temperature": 0.2,
            "max_output_tokens": 8192,
            "top_p": 0.95,
            "top_k": 40,
        }

        for attempt in range(self.max_quota_retries + 1):
            try:
                response = await chat_session.send_message_async(
                    prompt,
                    generation_config=generation_config
                )
                return response
            except google_exceptions.ResourceExhausted:
                if attempt < self.max_quota_retries:
                    wait_time = self.retry_delay_seconds
                    logger.warning(f"â³ [{operation_name}] Quota exceeded. Retrying in {wait_time}s...")
                    await asyncio.sleep(wait_time)
                else:
                    raise AIServiceError("quota_exceeded", f"Quota exceeded after {self.max_quota_retries} retries.")
            except Exception as e:
                logger.error(f"âŒ [{operation_name}] Failed: {e}")
                raise e
        raise AIServiceError("unknown_error", "Unexpected error in chat retry")

    # -------------------------------------------------------------------------
    # 2. ë‹¨ê³„ë³„ ì´ì–´ë°›ê¸° í—¬í¼ (Continuation)
    # -------------------------------------------------------------------------
    async def _handle_step_continuation(self, chat_session, initial_response, step_name):
        """ê° ë‹¨ê³„ ë‚´ì—ì„œ í† í°ì´ ì˜ë ¸ì„ ë•Œ ë¬¸ë§¥ì„ ìœ ì§€í•˜ë©° ì´ì–´ë°›ê¸°"""
        full_text = initial_response.text
        
        if not initial_response.candidates:
             return full_text

        finish_reason = initial_response.candidates[0].finish_reason
        
        attempt = 0
        # 2 = MAX_TOKENS
        while finish_reason == 2 and attempt < self.max_continuation_attempts:
            attempt += 1
            logger.warning(f"âš ï¸ {step_name} truncated (MAX_TOKENS). Continuation #{attempt}...")
            
            # ë¬¸ë§¥ ìœ ì§€ë¥¼ ìœ„í•´ ë§ˆì§€ë§‰ ì¤„ì„ í¬í•¨í•˜ì—¬ ìš”ì²­
            last_chars = full_text.strip()[-100:]
            continuation_prompt = (
                f"âš ï¸ SYSTEM: Previous response was truncated. "
                f"CONTINUE generating the code exactly from where it stopped:\n"
                f"...{last_chars}\n"
                f"(Do NOT repeat the context above. Just output the rest of the code.)"
            )
            
            response = await self._send_chat_with_retry(
                chat_session, 
                continuation_prompt, 
                operation_name=f"{step_name} Continuation #{attempt}"
            )
            
            part_text = response.text
            
            # ë§ˆí¬ë‹¤ìš´ ë¸”ë¡ ì œê±°
            if part_text.strip().startswith("```"):
                lines = part_text.strip().split('\n')
                if len(lines) > 0 and (lines[0].startswith("```") or "jsx" in lines[0] or "javascript" in lines[0]):
                     part_text = '\n'.join(lines[1:])
            if part_text.strip().endswith("```"):
                part_text = part_text.strip()[:-3]
            
            full_text += part_text
            
            if response.candidates:
                finish_reason = response.candidates[0].finish_reason
            else:
                break
            
        if finish_reason == 2:
            logger.error(f"âŒ {step_name} still truncated after max attempts.")
            
        return full_text

    # -------------------------------------------------------------------------
    # 3. ë©”ì¸ ìƒì„± í•¨ìˆ˜ (4ë‹¨ê³„ ìˆœì°¨ ì‹¤í–‰ + ì§„í–‰ë¥  ì½œë°±)
    # -------------------------------------------------------------------------
    async def generate_prototype(
        self, 
        menu_name: str,
        screen_name: str,
        wizard_data: Optional[Dict[str, Any]] = None,
        progress_callback: Optional[Callable[[int, str], Awaitable[None]]] = None
    ) -> Dict[str, str]:
        """Wizard ê¸°ë°˜ 4ë‹¨ê³„ ìˆœì°¨ì  ì½”ë“œ ìƒì„± (ì§„í–‰ë¥  ì½œë°± í¬í•¨)"""
        logger.info(f"ğŸš€ Starting 4-Stage Generation for: {menu_name}/{screen_name}")
        
        if not wizard_data:
            raise AIServiceError("missing_wizard_data", "Wizard data required.")

        # 1. ChatSession ì´ˆê¸°í™” (Context Caching ì ìš©)
        chat_session = None
        is_cached = False
        
        try:
            # ìºì‹œëœ ëª¨ë¸ ë˜ëŠ” ì¼ë°˜ ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
            model, is_cached = self._get_or_create_cached_model()
            
            if is_cached:
                # ìºì‹œëœ ëª¨ë¸ ì‚¬ìš© - SYSTEM_PROMPTê°€ ì´ë¯¸ í¬í•¨ë¨
                logger.info("âœ¨ Starting chat with CACHED context")
                chat_session = model.start_chat()
            else:
                # ì¼ë°˜ ëª¨ë¸ ì‚¬ìš© - SYSTEM_PROMPTë¥¼ historyì— í¬í•¨
                logger.info("ğŸ“ Starting chat with fresh context")
                chat_session = model.start_chat(history=[
                    {"role": "user", "parts": [SYSTEM_PROMPT, "í”„ë¡œì íŠ¸ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤."]},
                    {"role": "model", "parts": ["ë„¤, ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. React í”„ë¡œí† íƒ€ì… ìƒì„±ì„ ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤."]}
                ])
        except Exception as e:
            logger.error(f"âŒ Chat setup failed: {e}")
            # í´ë°±: ê¸°ë³¸ ëª¨ë¸ë¡œ ì‹œì‘
            chat_session = self.model.start_chat(history=[
                {"role": "user", "parts": [SYSTEM_PROMPT, "Start."]},
                {"role": "model", "parts": ["Ready."]}
            ])

        # 2. 4ë‹¨ê³„ í”„ë¡¬í”„íŠ¸ ì¤€ë¹„
        try:
            steps = [
                ("Step 1 (Utils)", get_step_1_prompt(wizard_data), 40, "ê¸°ì´ˆ ì„¤ì • ë° ìœ í‹¸ë¦¬í‹° ì •ì˜ ì¤‘..."),
                ("Step 2 (State)", get_step_2_prompt(wizard_data), 55, "ìƒíƒœ ê´€ë¦¬ ë° ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ êµ¬í˜„ ì¤‘..."),
                ("Step 3 (UI)", get_step_3_prompt(wizard_data), 75, "ë©”ì¸ í™”ë©´ UI ë Œë”ë§ ì¤‘..."),
                ("Step 4 (Modals)", get_step_4_prompt(wizard_data), 90, "ëª¨ë‹¬ íŒì—… ë° ìµœì¢… ì™„ì„± ì¤‘...")
            ]
        except Exception as e:
            raise AIServiceError("prompt_error", f"í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„± ì‹¤íŒ¨: {e}")

        full_prompt_log = f"=== [SYSTEM PROMPT] ===\n{SYSTEM_PROMPT}\n\n"

        # 3. ìˆœì°¨ì  ì‹¤í–‰
        generated_code_chunks = []
        
        for i, (step_name, step_prompt, progress_percent, user_message) in enumerate(steps):
            logger.info(f"ğŸ”„ Processing {step_name}...")
            
            full_prompt_log += f"=== [{step_name}] ===\n{step_prompt}\n\n"

            if progress_callback:
                try:
                    await progress_callback(progress_percent, user_message)
                except Exception as e:
                    logger.warning(f"âš ï¸ Progress callback failed: {e}")

            response = await self._send_chat_with_retry(chat_session, step_prompt, operation_name=step_name)
            
            full_step_text = await self._handle_step_continuation(chat_session, response, step_name)
            
            # ë§ˆí¬ë‹¤ìš´ ì •ì œ
            cleaned_text = full_step_text.strip()
            if cleaned_text.startswith("```"):
                lines = cleaned_text.split('\n')
                if len(lines) > 0 and (lines[0].startswith("```") or "jsx" in lines[0] or "javascript" in lines[0]):
                     cleaned_text = '\n'.join(lines[1:])
            if cleaned_text.endswith("```"):
                cleaned_text = cleaned_text[:-3]
            
            generated_code_chunks.append(cleaned_text.strip())
            logger.info(f"âœ… {step_name} Completed. Length: {len(cleaned_text)}")

        # 4. ê²°ê³¼ ë³‘í•© ë° í›„ì²˜ë¦¬
        final_code = "\n\n".join(generated_code_chunks)
        
        # ì¤‘ë³µ Hook ì„ ì–¸ ì œê±°
        hook_declarations = re.findall(r"const\s*{\s*useState.*}\s*=\s*React;", final_code)
        if len(hook_declarations) > 1:
            logger.warning(f"Found {len(hook_declarations)} duplicate hook declarations. Cleaning up...")
            first_declaration = hook_declarations[0]
            final_code = final_code.replace(first_declaration, "__HOOK_PLACEHOLDER__")
            for decl in hook_declarations:
                final_code = final_code.replace(decl, "")
            final_code = final_code.replace("__HOOK_PLACEHOLDER__", first_declaration)

        # Sanity Check
        if "export default function" not in final_code:
            logger.warning("âš ï¸ Final code might be incomplete (missing export default)")

        return {
            "prototype_html": final_code,
            "final_prompt": "4-Step Chat Process",
            "full_prompt": full_prompt_log
        }

# ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤
_ai_service_instance: Optional[AIService] = None

def get_ai_service() -> AIService:
    """AI ì„œë¹„ìŠ¤ ì‹±ê¸€í†¤ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    global _ai_service_instance
    if _ai_service_instance is None:
        _ai_service_instance = AIService()
    return _ai_service_instance